{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Curr4_LSTM Sentiment Analysi .ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
        "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42",
        "id": "GcyCrKCQsT0h",
        "colab_type": "text"
      },
      "source": [
        "**Vanilla codes Created by Peter Nagy February 2017 <br/>\n",
        "Growing Sets method implemented by Ã–mer Kurttekin April 2020** <br/>\n",
        "[Nagy's Github][1] <br/>\n",
        "[Kurttekin's Github][2] <br/><br/> \n",
        "[Nagy's Linkedin](https://www.linkedin.com/in/peternagyjob/) <br/>\n",
        "**Sentiment Analysis:** the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n",
        "\n",
        "Dataset (Kaggle): [First GOP Debate Twitter Sentiment][3]\n",
        "\n",
        "  [1]: https://github.com/nagypeterjob\n",
        "  [2]: https://github.com/Omerktn\n",
        "  [3]: https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
        "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243",
        "trusted": false,
        "id": "yVQrF8M5sT07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "import datetime\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
        "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5",
        "id": "mUcMSjk3sT1f",
        "colab_type": "text"
      },
      "source": [
        "Only keeping the necessary columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
        "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58",
        "trusted": false,
        "id": "E7DI9FXDsT1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/<data_directory>/\"\n",
        "data = pd.read_csv(path + 'GOP_twitter_sent.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
        "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20",
        "id": "iMOiachesT19",
        "colab_type": "text"
      },
      "source": [
        "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
        "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
        "trusted": false,
        "id": "lm2ZXzORsT2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "\"\"\"\n",
        "To balance positive and negative labels, I created a little filter,\n",
        "it limits the maximum number of label for each label type.\n",
        "\"\"\"\n",
        "\n",
        "limit_each_sentiment = 2000\n",
        "poscount,negcount = 0,0\n",
        "posdroplist, negdroplist = [],[]\n",
        "\n",
        "for idx,row in data.iterrows():\n",
        "  if row['sentiment'] == 'Positive':\n",
        "    if poscount < limit_each_sentiment:\n",
        "      poscount += 1\n",
        "    else:\n",
        "      posdroplist.append(idx)\n",
        "  else:\n",
        "    if negcount < limit_each_sentiment:\n",
        "      negcount += 1\n",
        "    else:\n",
        "      negdroplist.append(idx)\n",
        "\n",
        "print(negdroplist)\n",
        "data = data.drop(negdroplist + posdroplist)\n",
        "\n",
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
        "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89",
        "id": "vtuuWEKvsT2W",
        "colab_type": "text"
      },
      "source": [
        "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
        "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
        "trusted": false,
        "id": "EmWnCBrBsT2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
        "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162",
        "id": "aFLCkMv3sT2t",
        "colab_type": "text"
      },
      "source": [
        "Hereby I declare the train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
        "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07",
        "trusted": false,
        "id": "RovZsUDJsT2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpO5W1-Uxv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_losses(x_in, y_in, model):\n",
        "  loss_arr = []\n",
        "  for i in range(len(x_in)):\n",
        "    loss_arr.append(model.evaluate(x_in[i:i+1], y_in[i:i+1], batch_size=1, verbose=0)[0])\n",
        "  return loss_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mToDO1NxINx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_growing_sets(X_train, Y_train, X_test, Y_test, model, sets_method=None, \n",
        "                            data_limit=None, div=16, num_epochs=30, batch_size=128):\n",
        "  \"\"\"\n",
        "  sets_method:  (String) Name of the growing sets method you want the train with. e.g. \"SPL\",\"SPLI\",\"ROGS\",\"BASE\"\n",
        "  data_limit: (Int) If you dont want to feed the model with all of your data, you can limit it.\n",
        "  div:  (Int) How many pieces you want to split your data. Those pieces will add up together in every step.\n",
        "  num_epochs:  (Int) Epoch for each training set.\n",
        "  \"\"\"\n",
        "  custom_methods = [\"spl\", \"spli\", \"rogs\", \"base\"]\n",
        "\n",
        "  is_str = isinstance(sets_method, str)\n",
        "  if is_str:\n",
        "    sets_method = sets_method.lower()\n",
        "  if not is_str  or sets_method not in custom_methods:\n",
        "    print(\"Sets method '{}' cannot recognized.\".format(sets_method))\n",
        "    return\n",
        "\n",
        "  if not data_limit:\n",
        "    data_limit = len(X_train)\n",
        "\n",
        "  X_train, Y_train = X_train[:data_limit], Y_train[:data_limit]\n",
        "\n",
        "  print(\"Method: {} - Div: {} - Epoch: {}\".format(sets_method.upper(),k_div,num_epochs))\n",
        "  start_t = datetime.datetime.now().replace(microsecond=0)\n",
        "\n",
        "  slen = int(len(X_train)/k_div)\n",
        "  spl_accs,rspl_accs,rogs_accs,accs  = [],[],[],[]\n",
        "  sizes = []\n",
        "\n",
        "  if sets_method == \"spl\":\n",
        "    # Self Paced Learning\n",
        "    for i in range(k_div):\n",
        "      losses = get_losses(X_train, Y_train, model)\n",
        "      sorted_loss_indexes = np.argsort(losses)\n",
        "      \n",
        "      x_tmp_spc = []\n",
        "      y_tmp_spc = []\n",
        "      for i in range(i * slen + slen):\n",
        "        x_tmp_spc.append(X_train[sorted_loss_indexes[i]])\n",
        "        y_tmp_spc.append(Y_train[sorted_loss_indexes[i]])\n",
        "\n",
        "      x_spc = np.asarray(x_tmp_spc)\n",
        "      y_spc = np.asarray(y_tmp_spc)\n",
        "\n",
        "      hist = model.fit(x_spc, y_spc, epochs=num_epochs, shuffle=False, batch_size=batch_size, verbose=0)\n",
        "      print(\"Instant LEN: {}\".format(len(x_spc)))\n",
        "\n",
        "      inst_acc = model.evaluate(X_test, Y_test)[1]\n",
        "      print(\"Acc: {}\".format(inst_acc))\n",
        "      spl_accs.append(inst_acc)\n",
        "      sizes.append(len(x_spc))\n",
        "    print(\"{}_accs = {}\".format(sets_method, spl_accs))\n",
        "\n",
        "  elif sets_method == \"spli\":\n",
        "    # Reversed Self Paced Learning\n",
        "    for i in range(k_div):\n",
        "      losses = get_losses(X_train, Y_train, model)\n",
        "      sorted_loss_indexes = np.argsort(losses)[::-1]\n",
        "      \n",
        "      x_tmp_spc = []\n",
        "      y_tmp_spc = []\n",
        "      for i in range(i * slen + slen):\n",
        "        x_tmp_spc.append(X_train[sorted_loss_indexes[i]])\n",
        "        y_tmp_spc.append(Y_train[sorted_loss_indexes[i]])\n",
        "\n",
        "      x_spc = np.asarray(x_tmp_spc)\n",
        "      y_spc = np.asarray(y_tmp_spc)\n",
        "\n",
        "      hist = model.fit(x_spc, y_spc, epochs=num_epochs, shuffle=False, batch_size=batch_size, verbose=0)\n",
        "      print(\"Instant LEN: {}\".format(len(x_spc)))\n",
        "\n",
        "      inst_acc = model.evaluate(X_test, Y_test)[1]\n",
        "      print(\"Acc: {}\".format(inst_acc))\n",
        "      rspl_accs.append(inst_acc)\n",
        "      sizes.append(len(x_spc))\n",
        "    print(\"{}_accs = {}\".format(sets_method, rspl_accs))\n",
        "\n",
        "  elif sets_method == \"rogs\":\n",
        "    # Random Ordered Growing Datasets\n",
        "    for i in range(k_div):\n",
        "      x_part = X_train[: i * slen + slen]\n",
        "      y_part = Y_train[: i * slen + slen]\n",
        "      hist = model.fit(x_part, y_part, epochs=num_epochs, shuffle=False, batch_size=batch_size, verbose=0)\n",
        "      print(\"Instant LEN: {}\".format(len(x_part)))\n",
        "\n",
        "      inst_acc = model.evaluate(X_test, Y_test)[1]\n",
        "      print(\"Acc: {}\".format(inst_acc))\n",
        "      rogs_accs.append(inst_acc)\n",
        "      sizes.append(len(x_part))\n",
        "    print(\"{}_accs = {}\".format(sets_method, rogs_accs))\n",
        "\n",
        "  elif sets_method == \"base\":\n",
        "    # Baseline training\n",
        "    for _ in range(num_epochs):\n",
        "      hist = model.fit(X_train, Y_train, epochs=1, shuffle=True, batch_size=batch_size, verbose=0) \n",
        "\n",
        "      inst_acc = model.evaluate(X_test, Y_test)[1]\n",
        "      print(\"Acc: {}\".format(inst_acc))\n",
        "      accs.append(inst_acc)\n",
        "      sizes.append(len(X_train))\n",
        "    print(\"{}_accs = {}\".format(sets_method, accs))\n",
        "\n",
        "  end_t = datetime.datetime.now().replace(microsecond=0)\n",
        "  print(\"{}_sizes = {}\".format(sets_method, sizes))\n",
        "  print(\"Time elapsed: {}\".format(end_t - start_t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TQW9XhEGNw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10\n",
        "k_div=10\n",
        "batch_size = 400\n",
        "\n",
        "train_with_growing_sets(X_train, Y_train, X_test, Y_test, model, sets_method=\"SPLI\",\n",
        "                            data_limit=100, div=k_div, num_epochs=num_epochs, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtjoBI4wcdDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD MODEL\n",
        "name = \"model_name\"\n",
        "modelpath = path + \"/models/\" + name\n",
        "\n",
        "# Model reconstruction from JSON file\n",
        "with open(modelpath + \".json\", 'r') as f:\n",
        "    model = model_from_json(f.read())\n",
        "# Load weights into the new model\n",
        "model.load_weights(modelpath + \".h5\")\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAaCugAqyJ4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SAVE MODEL\n",
        "name = \"model_name\"\n",
        "modelpath = \"/<data_directory>/\" + name\n",
        "print(modelpath)\n",
        "model.save_weights(modelpath + \".h5\")\n",
        "with open(modelpath + \".json\", 'w') as f:\n",
        "    f.write(model.to_json())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
